# üöÄ M√≥dulo 2 - Proyecto Integrador - Avance 3
### üë®‚Äçüíª Presentado por:
## Luis Carlos Pacheco Lanzziano  
## Historial de Optimizaci√≥n ETL FleetLogix
Este archivo documenta los cambios cr√≠ticos realizados en el script A3-05_etl_pipeline_estudiantes.py para transformar un proceso ineficiente en una carga masiva de alto rendimiento.

- 1. Cambio de Paradigma: De Iterativo a Vectorizado

Cambio: Se eliminaron los bucles for index, row in df.iterrows() para c√°lculos de negocio.

Prop√≥sito: El uso de Pandas Vectorized Operations permite procesar 400,000 registros en la memoria RAM en milisegundos. Anteriormente, cada c√°lculo individual sumaba latencia innecesaria al tiempo total de ejecuci√≥n.

- 2. Implementaci√≥n de write_pandas (Bulk Loading)

Cambio: Sustituci√≥n de comandos INSERT individuales por la funci√≥n write_pandas de Snowflake.

Prop√≥sito: Esta herramienta comprime los datos en formato Parquet y los sube a un Stage temporal. Esto reduce el tiempo de carga de las dimensiones de horas a segundos, eliminando miles de peticiones peque√±as a la red.


- 3. Uso de executemany con chunk_size

Cambio: Los registros de la tabla de hechos se agrupan en lotes de 15,000 registros (chunk_size = 15000).

Prop√≥sito: Evita el "cuello de botella" de la red. En lugar de realizar 400,000 viajes a la base de datos, el script realiza aproximadamente 27 viajes masivos. Esto equilibra la velocidad de carga sin agotar la memoria RAM del sistema local.

- 4. Deduplicaci√≥n Agresiva en Memoria

Cambio: Aplicaci√≥n de .drop_duplicates() en los datos de dimensiones antes de tocar Snowflake.

Prop√≥sito: Si un cliente o veh√≠culo aparece miles de veces en los registros de entrega, el script ahora solo procesa el valor √∫nico una vez. Esto reduce dr√°sticamente el n√∫mero de operaciones MERGE necesarias.

- 5. Automatizaci√≥n de Infraestructura (Self-Healing)

Cambio: Creaci√≥n de los m√©todos setup_infrastructure y setup_time_dimension.

Prop√≥sito:

Secuencias: Garantiza que los generadores de IDs (seq_customer_key, etc.) existan antes de la carga.

Dimensi√≥n de Tiempo: Genera autom√°ticamente 86,400 registros (un d√≠a completo segundo a segundo) dentro de Snowflake usando un generador interno.

Tablas: Crea la tabla daily_performance_summary si no existe, asegurando que el c√°lculo de KPIs finales nunca falle.

- 6. Mapeo por Diccionarios (In-Memory Mapping)

Cambio: Las llaves for√°neas (IDs de clientes, conductores y veh√≠culos) se descargan a diccionarios de Python al inicio de la carga de hechos.

Prop√≥sito: Buscar una ID en un diccionario de Python es miles de veces m√°s r√°pido que realizar una consulta SELECT a Snowflake por cada fila procesada.

## Descripci√≥n de las Querys implementadas
- 1. Generaci√≥n de la Dimensi√≥n de Tiempo (dim_time)

Esta consulta utiliza un generador interno de Snowflake para crear datos sin necesidad de cargarlos desde un archivo externo:

generator(rowcount => 86400): Crea 86,400 filas instant√°neamente, una por cada segundo del d√≠a.

timeadd(second, seq4(), '00:00:00'): Calcula cada segundo exacto partiendo desde la medianoche.

L√≥gica de Negocio: Clasifica autom√°ticamente cada segundo en franjas como 'Ma√±ana', 'Tarde' o 'Noche' y determina si es horario laboral (is_business_hour).

time_key: Se genera combinando HHMMSS para crear una llave natural que facilita los cruces (JOINs) r√°pidos.

- 2. Sincronizaci√≥n de Dimensiones (MERGE)

En lugar de insertar registros uno por uno, se utiliza la sentencia MERGE para gestionar cambios:

WHEN NOT MATCHED THEN INSERT: Compara la tabla de Staging (datos nuevos) con la dimensi√≥n final. Si el cliente o veh√≠culo no existe, lo inserta.

Uso de Secuencias: Emplea seq_customer_key.NEXTVAL para asignar autom√°ticamente un identificador √∫nico y correlativo a cada nuevo registro.

- 3. Carga Masiva de Hechos (INSERT por lotes)

Para la tabla de hechos, donde reside el volumen de 400,000 registros, la query est√° dise√±ada para la velocidad:

Inserci√≥n Directa: Se listan todas las columnas expl√≠citamente para asegurar que los datos transformados en Python encajen perfectamente en la estructura de Snowflake.

etl_batch_id: Cada inserci√≥n marca los registros con un ID √∫nico de ejecuci√≥n, lo que permite auditar la carga y borrar lotes espec√≠ficos si hubo alg√∫n error en el proceso.

- 4. Resumen de Rendimiento (daily_performance_summary)

Esta consulta delega el c√°lculo matem√°tico a Snowflake en lugar de hacerlo en la memoria de tu computadora:

Agregaci√≥n SQL: Utiliza funciones como AVG, SUM y COUNT sobre el lote reci√©n cargado (WHERE etl_batch_id = %s).

Eficiencia: Procesar estas sumas directamente en la base de datos sobre 400k filas toma menos de un segundo, mientras que en Python requerir√≠a descargar y procesar todos los datos nuevamente.

- 5. Infraestructura de Soporte (CREATE IF NOT EXISTS)

CREATE SEQUENCE: Prepara los contadores autom√°ticos para las llaves primarias de las dimensiones.

CREATE TABLE: Garantiza que la tabla de reporte final exista antes de intentar guardar los resultados, evitando errores de "Tabla no encontrada".

## Documentaci√≥n de Seguridad y Autenticaci√≥n
Para que el pipeline ETL funcione de manera segura, se ha implementado una autenticaci√≥n basada en pares de llaves (Key-Pair Authentication), eliminando la necesidad de usar contrase√±as en texto plano.

1. Instalaci√≥n de Dependencias de Seguridad
Adem√°s de la integraci√≥n con Pandas, se requiere una librer√≠a espec√≠fica para manejar tokens de autenticaci√≥n de forma segura en el almacenamiento local:


`pip install "snowflake-connector-python[secure-local-storage]"`
Prop√≥sito: Guarda los tokens de sesi√≥n de forma cifrada en el sistema operativo, evitando re-autenticaciones constantes y protegiendo las credenciales.

2. Generaci√≥n del Par de Llaves (Criptograf√≠a)
El proceso utiliza el est√°ndar RSA de 2048 bits para generar una identidad digital √∫nica para el usuario del ETL. Ejecuta estos comandos en tu terminal (Git Bash recomendado):

A. Generar Llave Privada y P√∫blica (Formato PEM)

## Crea la llave privada
`openssl genrsa -out snowflake_key.pem 2048`

## Extrae la llave p√∫blica para subirla a Snowflake
openssl rsa -in snowflake_key.pem -pubout -out snowflake_key.pub
B. Conversi√≥n a Formato Binario (DER)
Snowflake Connector para Python requiere la llave en un formato espec√≠fico llamado DER para procesarla correctamente en el c√≥digo.


`openssl pkcs8 -topk8 -inform PEM -outform DER -in snowflake_key.pem -out snowflake_key.der -nocrypt`
C. Verificaci√≥n de Archivos
Aseg√∫rate de que el archivo .der se haya generado en la carpeta ra√≠z de tu proyecto:


`ls -la snowflake_key.der`
3. Implementaci√≥n en el C√≥digo
Una vez generados los archivos, el script FleetLogixETL utiliza el archivo snowflake_key.der para firmar digitalmente cada conexi√≥n:


Uso en Conexi√≥n: Se pasa como par√°metro private_key al m√©todo snowflake.connector.connect.
- lista de tablas creadas
![alt text](image-13.png)

- Muestra del historial de querys
![alt text](image-14.png)

- Muestra del modelo creado
![alt text](image-15.png)